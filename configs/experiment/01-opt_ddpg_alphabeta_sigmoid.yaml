# @package _global_
defaults:
  - base_optuna
  - override /tuner: ddpg
  - override /tuner/reward: sigmoidhybriddiscretmoveidelta
  - _self_

info: DDPG with Sigmoid reward and hyper params optim with Optuna + gamma with large variation [0.1,1] + fixed ALPHA=0.2 and BETA=0 + Optimized OBS (David) + cleaned WL (c98, Yif)

n_seeds: 4
n_jobs: 4 # Number of parallel trainings with different seed
n_trials: 80
hydra_launcher_n_jobs: 1 # Number of trials in parallel with Optuna

REW_ALPHA_COEF: 0.2
REW_BETA_COEF: 0


learning_rate: 0.001
buffer_size: 1000000
batch_size: 64
gamma: 0.99
tau: 0.005
noise_sigma: 0.1
train_freq: 1

hydra:
  sweeper:
    params:
      learning_rate: range(0.0005,0.009, step=0.0003)
      buffer_size: choice(500000,1000000,200000)
      batch_size: choice(64,128,256)
      gamma: range(0.01,1, step=0.05) # Lower value enforce reward values earlier (on first steps)
      tau: range(0.001, 1, step=0.005) # the soft update coefficient (“Polyak update”, between 0 and 1)
      noise_sigma: range(0.05,0.7, step=0.05)
      train_freq: range(1,40, step=8)


tuner:
  TRAINING_COVERAGE: 9
  agent:
    policy:
      learning_rate: ${learning_rate}
      buffer_size: ${buffer_size}
      batch_size: ${batch_size}
      gamma: ${gamma}
      tau: ${tau}
      action_noise:
        noise_sigma: ${noise_sigma}
      train_freq: ${train_freq}
