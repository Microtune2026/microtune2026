# @package _global_
defaults:
  - base_optuna
  - override /tuner: a2c
  - override /tuner/reward: continousv3E
  - _self_

info: A2C hyper params optim with Optuna + rew=continousv3E and variable ALPHA in [-0.1,1] and BETA in [0,1] +  Optimized OBS (David) + cleaned WL (c98, Yif)

n_seeds: 3
n_jobs: 3 # Number of parallel trainings with different seed
n_trials: 40
hydra_launcher_n_jobs: 4 # Number of trials in parallel with Optuna

REW_ALPHA_COEF: 0
REW_BETA_COEF: 0
learning_rate: 0.0003
gamma: 0.99
n_steps: 5

hydra:
  sweeper:
    params:
      REW_ALPHA_COEF: range(-0.1, 1, step=0.1)
      REW_BETA_COEF:  range(0, 1, step=0.1)
      learning_rate: range(0.0001,0.001, step=0.0001)
      gamma:  range(0.90,0.996, step=0.005) # Default 0.99. Lower value enforce reward values earlier (on first steps)
      n_steps: choice(3,5,10,15,20)



tuner:
  TRAINING_COVERAGE: 9
  agent:
    policy:
      learning_rate: ${learning_rate}
      gamma: ${gamma}
      n_steps:  ${n_steps}