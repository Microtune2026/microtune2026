# @package _global_
defaults:
  - base_optuna
  - override /tuner: dqn
  - _self_

# learning_rate=0.0001, buffer_size=1000000, learning_starts=100, batch_size=32, tau=1.0, gamma=0.99, train_freq=4, gradient_steps=1, 
# replay_buffer_class=None, replay_buffer_kwargs=None, optimize_memory_usage=False, target_update_interval=10000, 
# exploration_fraction=0.1, exploration_initial_eps=1.0, exploration_final_eps=0.05, max_grad_norm=10, stats_window_size=100, 
# tensorboard_log=None, policy_kwargs=None, verbose=0, seed=None, device='auto', _init_setup_model=True
learning_rate: 0.0005
batch_size: 64
train_freq: 4
gamma: 0.99

hydra:
  #sweep:
    #dir: /var/local/data/trainer/multirun/ppo_${now:%Y%m%d}-${now:%H%M%S}
  sweeper:
    params:
      learning_rate: range(0.0003,0.001, step=0.00003)
      batch_size: choice(32,64)
      train_freq: choice(2,4,8,16,64)
      gamma: range(0.90,0.996, step=0.005) # Default 0.99. Lower value enforce reward values earlier (on first steps)
#      DETERMINISTIC: choice(True,False)
#      gae_lambda: choice(0.90,0.95,0.98)
#      clip_range: choice(0.1,0.2,0.4,0.6)
#      noise_sigma: choice(0.1,0.2,0.3)
#      steps_success_count_threshold: choice(8,12)


tuner:
#  TRAINING_COVERAGE: 16 # Coverage of the total states present in dataset. 1.0 means that almost all the dataset will be covered (depends also on the random distribution that may repeat twice the same state)
#  TRAINING_STEPS_PER_EPISODE: 33    # Important to take benefit of the decreasing exploration vs explotation tradeoff
#  NORMALIZE_OBS: True
  agent:
    policy:
      learning_rate: ${learning_rate}
      batch_size: ${batch_size}
      train_freq: ${train_freq}
      gamma: ${gamma}
