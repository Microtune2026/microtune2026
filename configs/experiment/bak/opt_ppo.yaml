# @package _global_
defaults:
  - base_optuna
  - override /tuner: ppo
  - _self_

# learning_rate=0.0003, n_steps=2048, batch_size=64, n_epochs=10, gamma=0.99, gae_lambda=0.95, clip_range=0.2, clip_range_vf=None, normalize_advantage=True, 
# ent_coef=0.0, vf_coef=0.5, max_grad_norm=0.5, use_sde=False, sde_sample_freq=-1, rollout_buffer_class=None, rollout_buffer_kwargs=None, 
# target_kl=None, stats_window_size=100, 
# tensorboard_log=None, policy_kwargs=None, verbose=0, seed=None, device='auto', _init_setup_model=True
learning_rate: 0.0003
batch_size: 64
normalize_advantage: False
n_epochs: 10
gamma: 0.99
#use_sde: False  # Works only with continous action space

hydra:
  #sweep:
    #dir: /var/local/data/trainer/multirun/ppo_${now:%Y%m%d}-${now:%H%M%S}
  sweeper:
    params:
      learning_rate: range(0.0001,0.001, step=0.0001)
      batch_size: choice(32,64,128,256)
      normalize_advantage: choice(True,False)
      n_epochs: choice(3,6,12,24)
      gamma: range(0.8,0.99, step=0.05) # Lower value enforce reward values earlier (on first steps)
#      use_sde: choice(True,False)
#      DETERMINISTIC: choice(True,False)
#      gae_lambda: choice(0.90,0.95,0.98)
#      clip_range: choice(0.1,0.2,0.4,0.6)
#      steps_success_count_threshold: choice(8,12)


tuner:
  agent:
    policy:
      learning_rate: ${learning_rate}
      batch_size: ${batch_size}
      normalize_advantage: ${normalize_advantage}
      n_epochs: ${n_epochs}
#      use_sde: ${use_sde}
