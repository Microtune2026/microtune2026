# @package _global_
defaults:
  - common
  - override /tuner: linucb_kfoofw
  - override /tuner/env/observations: prod
#  - override /tuner/env/state_selector@tuner.env.state_selector_train: df_fullrandom
  - _self_

n_trials: 8 # job.num will be in range [0, n_trials-1]
#tuner.env.state_selector_train.sigma_tipping_ratio: 0.8 #NOT WORKING

learning_rate: 0.9

hydra:
  #sweep:
    #dir: /var/local/data/trainer/multirun/ppo_${now:%Y%m%d}-${now:%H%M%S}
  sweeper:
    params:
      learning_rate: range(0.3, 1.2, step=0.02)

tuner:
  NORMALIZE_OBS: False
  TRAINING_COVERAGE: 2 # Coverage of the total states present in dataset. 1.0 means that almost all the dataset will be covered (depends also on the random distribution that may repeat twice the same state)
  TRAINING_STEPS_PER_EPISODE: 1    # Important to take benefit of the decreasing exploration vs explotation tradeoff
  agent:
    policy:
      learning_rate: ${learning_rate}
