# @package _global_
defaults:
  - base_optuna
  - override /tuner: ddpg
  - _self_

learning_rate: 0.001
buffer_size: 1000000
batch_size: 64
gamma: 0.99
noise_sigma: 0.1
train_freq: 1

#learning_rate=0.001, buffer_size=1000000, learning_starts=100, batch_size=256, tau=0.005, gamma=0.99, train_freq=1, gradient_steps=1, action_noise=None, replay_buffer_class=None, replay_buffer_kwargs=None, optimize_memory_usage=False, tensorboard_log=None, policy_kwargs=None, verbose=0, seed=None, device='auto', _init_setup_model=True
hydra:
  #sweep:
    #dir: /var/local/data/trainer/multirun/ppo_${now:%Y%m%d}-${now:%H%M%S}
  sweeper:
    params:
      learning_rate: range(0.0005,0.003, step=0.0003)
      buffer_size: choice(500000,1000000,200000)
      batch_size: choice(128,256,512)
      gamma: choice(0.90,0.94,0.99,0.999) # Default 0.99. Lower value enforce reward values earlier (on first steps)
      noise_sigma: choice(0.05,0.1,0.2,0.3,0.4)
      train_freq: choice(1,3,5)
#      gae_lambda: choice(0.90,0.95,0.98)
#      clip_range: choice(0.1,0.2,0.4,0.6)
#      normalize_advantage: choice(True,False)
#      seed: choice(581, 22, 312)
#      steps_success_count_threshold: choice(8,12)


tuner:
  agent:
    policy:
      learning_rate: ${learning_rate}
      buffer_size: ${buffer_size}
      batch_size: ${batch_size}
      gamma: ${gamma}
      action_noise:
        noise_sigma: ${noise_sigma}
      train_freq: ${train_freq}
