# @package _global_
defaults:
  - base
  - override /tuner: a2c
  - _self_

lr: 0.0003
gamma: 0.99
n_steps: 5
gae_lambda: 1.

hydra:
  #sweep:
    #dir: /var/local/data/trainer/multirun/ppo_${now:%Y%m%d}-${now:%H%M%S}
  sweeper:
    params:
      lr: range(0.0001,0.001, step=0.0001)
      gamma:  range(0.90,0.996, step=0.005) # Default 0.99. Lower value enforce reward values earlier (on first steps)
      n_steps: choice(3,5,10,15,20,28)
      gae_lambda: range(0.90,1., step=0.01)

# learning_rate=0.0007, n_steps=5, gamma=0.99, gae_lambda=1.0, ent_coef=0.0, vf_coef=0.5, max_grad_norm=0.5, rms_prop_eps=1e-05, use_rms_prop=True, use_sde=False, sde_sample_freq=-1, rollout_buffer_class=None, rollout_buffer_kwargs=None, normalize_advantage=False, stats_window_size=100, tensorboard_log=None, policy_kwargs=None, verbose=0, seed=None, device='auto', _init_setup_model=True
tuner:
  TRAINING_COVERAGE: 30
  TRAINING_STEPS_PER_EPISODE: 300
  env:
    wrapper:
      on_terminate: 4
  agent:
    policy:
      learning_rate: ${lr}
      gamma: ${gamma}
      n_steps:  ${n_steps}
      gae_lambda: ${gae_lambda}

