# @package _global_
defaults:
  - base_optuna
  - override /tuner: ddpg
  - override /tuner/reward: continousv3E
  - _self_

info: DDPG hyper params optim with Optuna + rew=continousv3E and variable ALPHA in [-0.1,1] and BETA in [0,1] +  Optimized OBS (David) + cleaned WL (c98, Yif)

n_seeds: 3
n_jobs: 3 # Number of parallel trainings with different seed
n_trials: 40
hydra_launcher_n_jobs: 4 # Number of trials in parallel with Optuna

REW_ALPHA_COEF: 0
REW_BETA_COEF: 0
learning_rate: 0.001
buffer_size: 1000000
batch_size: 64
gamma: 0.99
noise_sigma: 0.1
train_freq: 1

hydra:
  sweeper:
    params:
      REW_ALPHA_COEF: range(-0.1, 1, step=0.1)
      REW_BETA_COEF:  range(0, 1, step=0.1)
      learning_rate: range(0.0005,0.003, step=0.0003)
      buffer_size: choice(500000,1000000,200000)
      batch_size: choice(128,256,512)
      gamma: choice(0.90,0.94,0.99,0.999) # Default 0.99. Lower value enforce reward values earlier (on first steps)
      noise_sigma: choice(0.05,0.1,0.2,0.3,0.4)
      train_freq: choice(1,3,5)
#      gae_lambda: choice(0.90,0.95,0.98)
#      clip_range: choice(0.1,0.2,0.4,0.6)
#      normalize_advantage: choice(True,False)
#      seed: choice(581, 22, 312)
#      steps_success_count_threshold: choice(8,12)


tuner:
  TRAINING_COVERAGE: 9
  agent:
    policy:
      learning_rate: ${learning_rate}
      buffer_size: ${buffer_size}
      batch_size: ${batch_size}
      gamma: ${gamma}
      action_noise:
        noise_sigma: ${noise_sigma}
      train_freq: ${train_freq}
