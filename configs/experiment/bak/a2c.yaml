# @package _global_
defaults:
  - base_optuna
  - override /tuner: a2c
  - _self_

lr: 0.0003
gamma: 0.99
n_steps: 5
move_idelta: 0

hydra:
  sweeper:
    params:
      lr: range(0.0001,0.001, step=0.0001)
      gamma:  range(0.90,0.996, step=0.005) # Default 0.99. Lower value enforce reward values earlier (on first steps)
      n_steps: choice(3,5,10,15,20)

# learning_rate=0.0007, n_steps=5, gamma=0.99, gae_lambda=1.0, ent_coef=0.0, vf_coef=0.5, max_grad_norm=0.5, rms_prop_eps=1e-05, use_rms_prop=True, use_sde=False, sde_sample_freq=-1, rollout_buffer_class=None, rollout_buffer_kwargs=None, normalize_advantage=False, stats_window_size=100, tensorboard_log=None, policy_kwargs=None, verbose=0, seed=None, device='auto', _init_setup_model=True
tuner:
  NORMALIZE_OBS: True
  agent:
    policy:
      learning_rate: ${lr}
      gamma: ${gamma}
      n_steps:  ${n_steps}