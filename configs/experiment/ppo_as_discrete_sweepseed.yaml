# @package _global_
defaults:
  - base
  - override /tuner: ppo
  - override /tuner/env: discrete_actions
  - override /tuner/reward: sigmoidhybriddiscretmoveidelta
  - _self_


info: PPO with fixed hyper-parameters and basic sweep on multiple seeds + Sigmoid reward +  Optimized OBS (Dav.) + cleaned WL (c98, Yif.)
n_seeds: 1   # Nb seeds per trials
n_jobs: 1 # Number of parallel trainings with different seed of a trial
hydra_launcher_n_jobs: 4 # Number of trials in parallel with Optuna

REW_ALPHA_COEF: 0.2
REW_BETA_COEF: 0

learning_rate: 0.0001
batch_size: 64
normalize_advantage: False
n_epochs: 24
gamma: 0.86

hydra:
  sweeper:
    params:
      RND_SEED: range(4242, 4252, step=1)
tuner:
  TRAINING_COVERAGE: 9
  agent:
    policy:
      discrete_arms_mode: True  # This choice must be done in conformance with the choosen Gym Env (discrete or continous) in configuration
      learning_rate: ${learning_rate}
      batch_size: ${batch_size}
      normalize_advantage: ${normalize_advantage}
      n_epochs: ${n_epochs}
      gamma: ${gamma}
#      use_sde: ${use_sde}
