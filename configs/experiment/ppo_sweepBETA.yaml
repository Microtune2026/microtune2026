# @package _global_
defaults:
  - base
  - override /tuner: ppo
  - override /tuner/reward: sigmoidhybriddiscretmoveidelta
  - _self_

version_minor: 57   # To fix an error 

info: PPO with fixed hyper-parameters and basic sweep on multiple BETA coef (on STAY arm) + Sigmoid reward +  Optimized OBS (Dav.) + cleaned WL (c98, Yif.)
n_seeds: 4   # Nb seeds per trials
n_jobs: 4 # Number of parallel trainings with different seed of a trial
hydra_launcher_n_jobs: 3 # Number of trials in parallel with Optuna

REW_ALPHA_COEF: 0.2
REW_BETA_COEF: 0

# Best: (learning_rate=0.0005 batch_size=128 normalize_advantage=True n_epochs=3 gamma=0.81)
learning_rate: 0.0005
batch_size: 128
normalize_advantage: True
n_epochs: 3
gamma: 0.81

hydra:
  sweeper:
    params:
      REW_BETA_COEF: range(0, 1.04, step=0.1)
tuner:
  TRAINING_COVERAGE: 9
  agent:
    policy:
      learning_rate: ${learning_rate}
      batch_size: ${batch_size}
      normalize_advantage: ${normalize_advantage}
      n_epochs: ${n_epochs}
      gamma: ${gamma}
#      use_sde: ${use_sde}
