# @package _global_
defaults:
  - base_optuna
  - override /tuner: sac
  - override /tuner/reward: sigmoidhybriddiscretmoveidelta
  - _self_

info: SAC with Sigmoid reward and hyper params optim with Optuna + fixed ALPHA=0.2 and BETA=0 + Optimized OBS (David) + cleaned WL (c98, Yif)

n_seeds: 3
n_jobs: 3 # Number of parallel trainings with different seed
n_trials: 80
hydra_launcher_n_jobs: 4 # Number of trials in parallel with Optuna

REW_ALPHA_COEF: 0.2
REW_BETA_COEF: 0

#learning_rate=0.0003, buffer_size=1000000, learning_starts=100, batch_size=256, tau=0.005, gamma=0.99, train_freq=1, gradient_steps=1, 
#action_noise=None, replay_buffer_class=None, replay_buffer_kwargs=None, 
#optimize_memory_usage=False, ent_coef='auto', target_update_interval=1, target_entropy='auto', 
#use_sde=False, sde_sample_freq=-1, use_sde_at_warmup=False, stats_window_size=100, 
#tensorboard_log=None, 
learning_rate: 0.0003
buffer_size: 1000000
batch_size: 256
gamma: 0.99
train_freq: 1
gradient_steps: 1

hydra:
  sweeper:
    params:
      learning_rate: range(0.00007,0.001, step=0.00005)
      buffer_size: choice(500000,1000000,2000000)
      batch_size: choice(64,128,256,512)
      gamma: range(0.90,0.996, step=0.005) # Default 0.99. Lower value enforce reward values earlier (on first steps)
      train_freq: range(1,66, step=6)
      gradient_steps: choice(-1,1,3,6)



tuner:
  TRAINING_COVERAGE: 9
  agent:
    policy:
      learning_rate: ${learning_rate}
      buffer_size: ${buffer_size}
      batch_size: ${batch_size}
      gamma: ${gamma}
      train_freq:  ${train_freq}
      gradient_steps: ${gradient_steps}