# @package _global_
defaults:
  - base_optuna
  - override /tuner: sac
  - override /tuner/reward: sigmoidhybriddiscretmoveidelta
  - _self_

info: SAC with Sigmoid reward and hyper params optim with Optuna + gamma with large variation [0.1,1] + fixed ALPHA=0.2 and BETA=0 + Optimized OBS (David) + cleaned WL (c98, Yif)

n_seeds: 4
n_jobs: 4 # Number of parallel trainings with different seed
n_trials: 80
hydra_launcher_n_jobs: 2 # Number of trials in parallel with Optuna


REW_ALPHA_COEF: 0.2
REW_BETA_COEF: 0

#learning_rate=0.0003, buffer_size=1000000, learning_starts=100, batch_size=256, tau=0.005, gamma=0.99, train_freq=1, gradient_steps=1, 
#action_noise=None, replay_buffer_class=None, replay_buffer_kwargs=None, 
#optimize_memory_usage=False, ent_coef='auto', target_update_interval=1, target_entropy='auto', 
#use_sde=False, sde_sample_freq=-1, use_sde_at_warmup=False, stats_window_size=100, 
#tensorboard_log=None, 
learning_rate: 0.0003
buffer_size: 1000000
batch_size: 256
gamma: 0.99
tau: 0.005
train_freq: 1
noise_sigma: 0.01
gradient_steps: 1

hydra:
  sweeper:
    params:
      learning_rate: range(0.00007,0.001, step=0.00005)
      buffer_size: choice(500000,1000000,2000000)
      batch_size: choice(64,128,256,512)
      gamma: range(0.01,1, step=0.05) # Lower value enforce reward values earlier (on first steps)
      tau: range(0.001, 1, step=0.005) # the soft update coefficient (“Polyak update”, between 0 and 1)
      train_freq: range(1,40, step=8)
      noise_sigma: range(0.01,0.7, step=0.05)
      gradient_steps: choice(-1,1,3,6,10,16)



tuner:
  TRAINING_COVERAGE: 9
  agent:
    policy:
      learning_rate: ${learning_rate}
      buffer_size: ${buffer_size}
      batch_size: ${batch_size}
      gamma: ${gamma}
      tau: ${tau}
      train_freq:  ${train_freq}
      action_noise:
        noise_sigma: ${noise_sigma}
      gradient_steps: ${gradient_steps}