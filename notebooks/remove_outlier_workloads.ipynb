{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "def loadFromPickle(fullname):\n",
    "    try:\n",
    "        return pd.read_pickle(fullname, compression={'method': 'gzip'})\n",
    "    except Exception:\n",
    "        return pd.read_pickle(fullname)\n",
    "\n",
    "def saveToPickle(fullname, df):\n",
    "    df.to_pickle(fullname, compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1})\n",
    "\n",
    "def loadTrainTests(name, version):\n",
    "    df_train = loadFromPickle(name+'_train_'+version+\".pickle\")\n",
    "    df_test = loadFromPickle(name+'_test_'+version+\".pickle\")\n",
    "    return df_train, df_test\n",
    "\n",
    "def mergeTrainTest(name, version):\n",
    "    df_train, df_test = loadTrainTests(name, version)\n",
    "    return pd.concat([df_train, df_test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mergeTrainTest(\"./workloads\", \"9\")\n",
    "#PERF_OBJS = [0.9, 0.95, 0.965, 0.98, 0.99, 0.995, 0.997]\n",
    "data = data[data['perf_target_level'] == 0.98]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36864    0.98\n",
      "36865    0.98\n",
      "36866    0.98\n",
      "36867    0.98\n",
      "36868    0.98\n",
      "         ... \n",
      "98299    0.98\n",
      "98300    0.98\n",
      "98301    0.98\n",
      "98302    0.98\n",
      "98303    0.98\n",
      "Name: perf_target_level, Length: 15360, dtype: float64\n",
      "_______________\n",
      "37056    7.520286\n",
      "Name: sysbench_filtered.latency_mean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(data['perf_target_level'])\n",
    "print(\"_______________\")\n",
    "df = data[data['combined_column'] == '5 0.1 2 Ga']\n",
    "#df = df[df['observation.normalized_buf_size'] == 1]\n",
    "df = df[df['observation.innodb_buffer_pool_size'] == 8589934592]\n",
    "print(df['sysbench_filtered.latency_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_file_paths = [\"/home/cloud/poc_fanfan/vstune-bandits/df_eval.pkl\"]\n",
    "pickle_file_paths = [\"/home/cloud/src/app/workloads_test_11.pickle\", \"/home/cloud/src/app/workloads_train_11.pickle\"]\n",
    "pickle_file_paths_to_save = [\"/home/cloud/src/app/workloads_c098_test_11.pickle\", \"/home/cloud/src/app/workloads_c098_train_11.pickle\"]\n",
    "\n",
    "\n",
    "for idx, pickle_file_path in enumerate(pickle_file_paths):\n",
    "    print(f'Loading {pickle_file_path}...')\n",
    "    #with open(pickle_file_path, \"rb\") as f:\n",
    "        #data = pickle.load(f)\n",
    "    data = loadFromPickle(pickle_file_path)\n",
    "\n",
    "    print(\"Initial df length \", len(data))\n",
    "\n",
    "    # Group by the combination of interest\n",
    "    grouped = data.groupby([\"tables_rows\", \"wl_clients\", \"randtype\", 'tables', 'db_size_mb'])\n",
    "\n",
    "    # Create a dictionary that maps each combination to a sub-DataFrame\n",
    "    grouped_data = {}\n",
    "    for (tbl_rows, wl_cli, rtype, tbs, dbsz), grp_df in grouped:\n",
    "        grouped_data[(tbl_rows, wl_cli, rtype, tbs, dbsz)] = grp_df\n",
    "\n",
    "    iperf_sla = 0.98\n",
    "    indices_to_remove = []\n",
    "\n",
    "    for combo, df_sub in grouped_data.items():\n",
    "        # Sort by buf_size to define a clear \"time-like\" or ascending order\n",
    "        df_sub_sorted = df_sub.sort_values(\"buf_size\", ascending=True)\n",
    "\n",
    "        # Create boolean masks\n",
    "        above_098 = df_sub_sorted[\"iperf01\"] >= iperf_sla\n",
    "        below_098 = df_sub_sorted[\"iperf01\"] <= iperf_sla\n",
    "\n",
    "        # Check if there's at least one point above 0.98 and one point below 0.98\n",
    "        if above_098.any() and below_098.any():\n",
    "            first_above_idx = above_098[above_098].index.max()\n",
    "            last_below_idx = below_098[below_098].index.min()\n",
    "\n",
    "            # Convert these indices to their position in df_sub_sorted\n",
    "            first_above_order = df_sub_sorted.index.get_loc(first_above_idx)\n",
    "            last_below_order = df_sub_sorted.index.get_loc(last_below_idx)\n",
    "\n",
    "            # Check if the \"above 0.98\" event comes before the \"below 0.98\" event\n",
    "            if first_above_order < last_below_order:\n",
    "                # Mark these rows for removal\n",
    "                print(f\"               num rows    num tables    distribution   num cons    size db\")\n",
    "                print(f\"Combination to kill: {combo}\")\n",
    "                # print(df_sub_sorted[[\"buf_size\", \"iperf01\", \"db_size_mb\", \"tables\"]])\n",
    "                print(df_sub_sorted.index.tolist())\n",
    "                print(\"----------\")\n",
    "                indices_to_remove.extend(df_sub_sorted.index.tolist())\n",
    "\n",
    "    # Remove the rows that meet the condition\n",
    "    data_cleaned = data.drop(indices_to_remove)\n",
    "\n",
    "    print(\"Cleaned df length \", len(data_cleaned), \" vs. Initial \", len(data))\n",
    "\n",
    "    print(f'Save cleaned data: {pickle_file_paths_to_save[idx]}')\n",
    "    saveToPickle(pickle_file_paths_to_save[idx], data_cleaned)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
