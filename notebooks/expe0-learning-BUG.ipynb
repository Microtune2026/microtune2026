{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinUCB \n",
    "\n",
    "## Principle\n",
    "\n",
    "+ Learn LinUCB with or without SLA protect (reactive) option,\n",
    "+ The aim is to avoid Latency mean metric in learning and test context of LinUCB\n",
    "+ Test LinUCB without SLA protect when Latency is NOT in context, use SLA protect if Latency is in context\n",
    "+ HPA and Basic Baseline tests can or not use SLA protect and Latency metric in context. \n",
    "+ Buffer Fillg Rate (aka BFR) policy, is another baseline. It don't need to know the Latency. Up to you to activate or not SLA protect mode in Bas\n",
    "+ Basic policy particularities:\n",
    "  +  needs the latency and acceptable gap information\n",
    "  +  oscillates around the tipping point when the slope of the latency is stiff\n",
    "+  HPA policy particularities:\n",
    "   +  needs latency information\n",
    "+ Training is performed with Episodes of 1 step. At each episode a workload is choosen and a state (with a buffer value) is choosen randomly. The ON_TERMINATE option of the environmenent does not make sense here.\n",
    "\n",
    "## Results\n",
    "\n",
    "+ La normalisation (minmax=[0,1]) de l'espace d'observation donnent de mauvais résultats dés l'entrainement\n",
    "+ Si on définit une ou des actions négatives, l'entrainement sans le mod REACT fonctionne mieux, car sinon on n'apprend pas bien sur le bras UP\n",
    "+ Autant les erreurs de réglage dans la plage OVER s'avère acceptables, il faut absolument trouver un moyen d'éviter les VIOLATION\n",
    "+ Résolu? \n",
    "  Si ALPHA est trop petit (0.3 et moins), on a une erreur: \n",
    "```  \n",
    "  chosen_arm = np.random.choice(candidate_arms)\n",
    "     93 return chosen_arm\n",
    "  File mtrand.pyx:934, in numpy.random.mtrand.RandomState.choice()\n",
    "```  \n",
    "\n",
    "### Labo mode\n",
    "\n",
    "+ REACT à True (Training and Test)\n",
    "  + LinUCB Train CReg: **359**\n",
    "  + KFOOFW LinUCB CReg:**10.64**  => C'est très bon!\n",
    "  + BFR CReg:1758.59\n",
    "  + Hpa CReg:546.45\n",
    "  + Basic CReg: **116.43**\n",
    "\n",
    "+ REACT à False (Training) and True (Test). \n",
    "  + LinUCB Train CReg: **493**\n",
    "  + KFOOFW LinUCB CReg: **135.12**\n",
    "  + BFR CReg:1758.59\n",
    "  + Hpa CReg:546.45\n",
    "  + Basic CReg:116.43\n",
    "  + Les plus mauvais scores (forcément en descendant!!):\n",
    "    + E28  30 1.0 4 Un 0.98 20.0ms DBSize:7052MB Dft DBA:5632.0MB  CREG: 35\n",
    "      + En mode descendant. On s'arrête trop top, assez loin de la cible, pourquoi ? => regarder les données\n",
    "    + E90  22 1.0 12 Pa 0.98 20.0ms DBSize:5233MB Dft DBA:4096.0MB CREG: 7\n",
    "      + En mode descendant. On s'arrete trop top, pourquoi ? => regarder les données\n",
    "\n",
    "### Prod mode\n",
    "\n",
    "+ REACT à False (Training and Test)\n",
    "  + LinUCB Train CReg: **606** (then 584)\n",
    "  + KFOOFW LinUCB CReg: **223.79** (then 199)\n",
    "  + BFR CReg: 2316.84\n",
    "  + Les plus mauvais scores:\n",
    "    + ['E14 S63 8192.0MB', 34.7158, **1.2138**, 0], ['E15 S63 128.0MB', 4.714, **35.726**, 0] E15  15 1.0 6 Sp 0.98 20.0ms DBSize:3540MB Dft DBA:2816.0MB\n",
    "      + E14 On descend un peu trop mais on est bien\n",
    "      + E15 On cherche à descendre encore mais on est bien\n",
    "    + ['E19 S63 128.0MB', 5.8779, 30.0268, 0] E19  22 1.0 3 Sp 0.98 20.0ms DBSize:5167MB Dft DBA:4096.0MB\n",
    "      + On devrait remonter un peu mais rien de grave (SLA_OK)\n",
    "    + [['E92 S63 8192.0MB', 25.2901, 9.1486, 0], ['E93 S63 128.0MB', 3.0087, 7.7973, 0]] E92  40 0.3 8 Un 0.98 20.0ms DBSize:2867MB Dft DBA:2176.0MB\n",
    "      + On reste  à OVER en descente, donc c'est un \"mauvais\" réglage\n",
    "      + A la remontée, on reste à **VIOLATION** => on est très mauvais\n",
    "    + Idem E91  22 1.0 12 Pa 0.98 20.0ms DBSize:5233MB Dft DBA:4096.0MB, **VIOLATION**\n",
    "    + ['E94 S63 8192.0MB', 3.8052, 22.1507, 0], ['E95 S63 128.0MB', -37.5559, 75.1117, 0] E95  40 1.0 8 Pa 0.98 20.0ms DBSize:9441MB Dft DBA:7552.0MB\n",
    "      + BFR et HPA-Buf sont très mauvais à BUF=128MB. Il s'agit surement d'un pb de mesure prise pendant que le buffer était réorganisé. Le filling rate trop faible (89%), l'ago décide de rester à ce niveau au lieu de remonter\n",
    "      + Au final, même lorsque la mesure est prise dans de mauvaise condition, le contexte plus large de LinUCB, permet de mieux fonctionner\n",
    "  + Globalement, on peut dire que les résultats ne sont pas mauvais, mais juste moins précis (pb de la zone OK) que en mode LABO\n",
    "\n",
    "+ REACT à True (Training) and False (Test). \n",
    "  + LinUCB Train CReg: **409**\n",
    "  + KFOOFW LinUCB CReg: **1442.19**\n",
    "  + BFR CReg:2316.84\n",
    "  + Les plus mauvais scores: Beaucoup, car on n'a pas appris à remonter\n",
    "  + **Ce UC n'a aucun sens**\n",
    "\n",
    "\n",
    "+ Conclusion:\n",
    "  + Il faut un indicateur de \"réglage satisfaisant\" car on accumule du regret parfois dans des conditions pourtant OK\n",
    "  + Le mode REACT améliore l'apprentissage mais il n'est pas pertinent si on ne peut pas l'utiliser en TEST\n",
    "\n",
    "\n",
    "## Notes HSLinUCB\n",
    "\n",
    "+ Donne de moins bons résultats pour l'instant que KFOO\n",
    "\n",
    "## Notes KFOOLinUCB\n",
    "\n",
    "+ Un ALPHA=0.5 est pas mal, COVERAGE=2, FullSequential DSSelector\n",
    "+ On a observé un modèle ayant obtenu un meilleur (de peu) résultat d'apprentissage qui performe moins bien en test\n",
    "+ Questionnement sur Rewards V3 vs V3E...\n",
    "+ Le training en mode aléatoire (Gaussien) ne se différentie pas beaucoup du mode séquentiel\n",
    "+ Fort impact de TRAINING_USE_SLA_PROTECT à True (meilleure). On considère qu'on peut utiliser la latence pendant l'entrainement pour le calcul de Reward. \n",
    "+ Ne pas activer TEST_USE_SLA_PROTECT, si on n'a pas la latence du client après déploiement en Prod.\n",
    "+ Usage ou pas de la latence dans l'observation. Attention, si on l'utilise, on devra aussi l'avoir après déploiement de l'agent en Production!!!\n",
    "+ A noter que parfois des zones \"OK\" sont coincées entre 2 zones \"OVER\". Le regret cumulé en est impacté. \n",
    "+ Qq constat dans la config de learning avec ds_train3, TRAINING_SLA_PROTECT=True et TEST_SLA_PROTECT=False, Reward=ADBMSBufferCacheRewardContinous. \n",
    "  + Si 10, 12 clients sont dans le dataset, la performance du modèle se dégrade pour ces 2 cas\n",
    "  + En Test:\n",
    "    + On voit que on a  un regret qui suit le regret HPA. C'est normal vu notre calcul de Reward (Continous @Pat)?\n",
    "    + Si on fait des tests de descente de buffer uniquement, les résultats sont bien meilleurs que les tests avec descente + montée. Les tests de montée seuls suivent majoritairement le regret HPA ou font mieux.\n",
    "  + Essais aux différentes valeurs de perf_level (default=0.98, Toutes: [0.9   0.95  0.965 0.98  0.99  0.995 0.997]):\n",
    "    + A perf maximale, c'est la catastrophe, ça ne marche pas, l'apprentissage a échoué\n",
    "    + C'est plus simple d'apprendre à niveau de perf plus faible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.pardir))\n",
    "\n",
    "import bandits.datasource.adbms_dataframe as ds\n",
    "import bandits.reward as rw\n",
    "from bandits.gym_env import VSEnv, VSMonitor\n",
    "from bandits.agent import VSAgent\n",
    "\n",
    "# Parameters\n",
    "RND_SEED = 4242\n",
    "#qpslat_weights=\"01\"   # QPS is not used in performance computation (weigth is 0%), the objective is to maintain te Latency (weigth is 100%). Other options is \"19\" (10% for QPS, 90% for Latency)\n",
    "\n",
    "PLOTLY_RENDERER=\"svg\" # \"plotly_mimetype+notebook\": Ex: png, jpg, svg\n",
    "IPERF_LEVEL=0.98    # [0.9   0.95  0.965 0.98  0.99  0.995 0.997]\n",
    "VERSION=9\n",
    "\n",
    "# Choose ratio=80 for 80% train and 20% test. Choose 0 for a separation by clients. Ex Train on all Odd wl_clients num and test on even wl_clients bum\n",
    "datasets = ds.TrainTestDataSets(testastrain=False, ratio=0, version=VERSION, perf_level=IPERF_LEVEL, randtypes=[ \"special\", \"gaussian\", \"uniform\", \"pareto\" ], clients=[1,2,3,4,5,6,7,8,9,10,11,12], seed=42, verbose=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OBSERVATION_SPACE_ELEMS = [\n",
    "####    \"tables\",                      # Not significant while in production the DB schema is totaly different than the one built by Sysbench. DB Size should be sufficient\n",
    "####    \"tables_rows\",                 # Not significant while in production the DB schema is totaly different than the one built by Sysbench. DB Size should be sufficient\n",
    "#    \"iperf01\",                      # Current performance indicator based on WPS and Latency, 01 is computed with weigths as follow: 0% QPS and 100% Latency\n",
    "    \"db_size_mb\",                    # Initial DB size after creation of all tables+indexes and before the workload with all clients\n",
    "    \"sysbench_filtered.latency_mean\",      # [0, 1000] ms. Filtered latency from client. In inference/live mode, it should be a predicted value (see predictor.ipynb using XGBoost)\n",
    "#    \"buf_size_idx\",                      # Index form of the buffer size value [0, 63]\n",
    "    \"observation.normalized_buf_size\",     # [0., 1.] Taille normalisée courante du buffer cache\n",
    "#    \"perf_target_level\",           # [0., 1.] Objectif de performance cible. Est une valeur fixe lors des entrainements (1 modèle pour 1 objectif de perf)\n",
    "#    \"latency_threshold\",           # In ms. Est une valeur fixe lors des entrainements\n",
    "    \"observation.cache_used_pages_ratio\",\n",
    "    \"observation.cache_hit_ratio\",\n",
    "    \"extra_info.usage.db_size_initial.IndexSizeMB\", # Initial Indexes DB size \n",
    "    \"extra_info.usage.qps\",          # QPS picked on server (\"Questions\" Global status variable throughput)\n",
    "    \"extra_info.global_status.KBps\", # Kilo Bytes per seconds picked on server with \"Bytes_sent\" Global status variable throughput\n",
    "    \"observation.write_wait_ratio\",  # Seems not relevant and usable on MariaDB\n",
    "#    \"extra_info.global_status.threads_running_start\", # NOT POWERFULL!!?? The number of connections currently running an SQL query. Be carefull of a Bug in some MariaDB versions!! => TOO MUCH NOISE ON ITS VALUE!!!!!\n",
    "    \"extra_info.global_status.created_tmp_tables_diff\", # ?\n",
    "    \"extra_info.global_status.handler_delete_diff\", # ?\n",
    "    \"extra_info.global_status.handler_update_diff\", # ?\n",
    "    \"extra_info.global_status.handler_write_diff\", # ?\n",
    "    \"extra_info.global_status.innodb_buffer_pool_read_requests_diff\",\n",
    "    \"extra_info.global_status.innodb_buffer_pool_write_requests_diff\",\n",
    "    \"extra_info.global_status.innodb_buffer_pool_wait_free_diff\",\n",
    "    \"extra_info.global_status.innodb_data_reads_diff\",\n",
    "    \"extra_info.global_status.innodb_data_written_diff\",\n",
    "    \"extra_info.global_status.max_used_connections_end\",\n",
    "    \"extra_info.global_status.threads_running_end\",\n",
    "    \"extra_info.global_status.threads_connected_end\",\n",
    "    \"extra_info.global_status.innodb_buffer_pool_bytes_dirty_end\", # Seems cool to have it\n",
    "    \"extra_info.global_status.innodb_buffer_pool_pages_free_end\",  # Seems cool to have it\n",
    "#    \"extra_info.global_status.innodb_buffer_pool_pages_data_diff\",\n",
    "    \"extra_info.global_status.innodb_buffer_pool_bytes_dirty_diff\", # Seems cool to have it\n",
    "    \"extra_info.global_status.innodb_buffer_pool_pages_free_diff\",  # Seems cool to have it\n",
    "    \"extra_info.global_status.information_schema.innodb_metrics.lock_row_lock_time_max\",\n",
    "    \"extra_info.global_status.information_schema.innodb_metrics.lock_row_lock_waits\",\n",
    "    \"extra_info.global_status.information_schema.innodb_metrics.os_pending_reads\",\n",
    "    \"extra_info.global_status.information_schema.innodb_metrics.os_pending_writes\",\n",
    "    \"extra_info.global_status.information_schema.innodb_metrics.file_num_open_files\",\n",
    "    \"extra_info.global_status.information_schema.innodb_buffer_pool_stats.MODIFIED_DATABASE_PAGES\", # Sounds COOL to have it!!\n",
    "    \"extra_info.global_status.information_schema.innodb_buffer_pool_stats.PAGES_READ_RATE\",\n",
    "    \"extra_info.global_status.information_schema.innodb_buffer_pool_stats.PAGES_CREATE_RATE\",\n",
    "#    \"extra_info.global_status.information_schema.innodb_buffer_pool_stats.PAGES_WRITTEN_RATE\",  # CATASTROPHIC at an incredible point !!!\n",
    "    \"extra_info.global_status.information_schema.innodb_buffer_pool_stats.READ_AHEAD_RATE\",\n",
    "    \"extra_info.global_status.information_schema.innodb_buffer_pool_stats.READ_AHEAD_EVICTED_RATE\",\n",
    "    \"extra_info.global_status.information_schema.innodb_buffer_pool_stats.PAGES_MADE_YOUNG_RATE\", # Sounds COOL to have it!!\n",
    "    \"extra_info.global_status.information_schema.innodb_buffer_pool_stats.POOL_SIZE\", # Sounds COOL to have it!!\n",
    "    \"extra_info.global_status.information_schema.innodb_buffer_pool_stats.FREE_BUFFERS\",\n",
    "    \"extra_info.global_status.information_schema.innodb_buffer_pool_stats.DATABASE_PAGES\",\n",
    "    \"extra_info.global_status.information_schema.innodb_buffer_pool_stats.OLD_DATABASE_PAGES\",\n",
    "    \"extra_info.global_status.information_schema.innodb_buffer_pool_stats.PENDING_DECOMPRESS\",       #?\n",
    "    \"extra_info.global_status.information_schema.innodb_buffer_pool_stats.PENDING_READS\",       #?\n",
    "    \"extra_info.global_status.information_schema.innodb_buffer_pool_stats.PENDING_FLUSH_LRU\",       #?\n",
    "    \"extra_info.global_status.information_schema.innodb_buffer_pool_stats.PENDING_FLUSH_LIST\",       #?\n",
    "    \"extra_info.global_status.information_schema.innodb_buffer_pool_stats.PAGES_MADE_YOUNG\",\n",
    "    \"extra_info.global_status.information_schema.innodb_buffer_pool_stats.PAGES_NOT_MADE_YOUNG\",\n",
    "    \"extra_info.global_status.information_schema.innodb_buffer_pool_stats.PAGES_MADE_NOT_YOUNG_RATE\",\n",
    "    \"extra_info.global_status.information_schema.innodb_buffer_pool_stats.NUMBER_PAGES_READ\",       #?\n",
    "    \"extra_info.global_status.information_schema.innodb_buffer_pool_stats.NUMBER_PAGES_CREATED\",    #?\n",
    "    \"extra_info.global_status.information_schema.innodb_buffer_pool_stats.NUMBER_PAGES_WRITTEN\",    # ? \n",
    "    \"extra_info.global_status.information_schema.innodb_buffer_pool_stats.NUMBER_PAGES_GET\",\n",
    "    \"extra_info.global_status.information_schema.innodb_buffer_pool_stats.HIT_RATE\",\n",
    "    \"extra_info.global_status.information_schema.innodb_buffer_pool_stats.YOUNG_MAKE_PER_THOUSAND_GETS\",         #?\n",
    "    \"extra_info.global_status.information_schema.innodb_buffer_pool_stats.NOT_YOUNG_MAKE_PER_THOUSAND_GETS\",         #?\n",
    "    \"extra_info.global_status.information_schema.innodb_buffer_pool_stats.NUMBER_PAGES_READ_AHEAD\",         #?\n",
    "    \"extra_info.global_status.information_schema.innodb_buffer_pool_stats.NUMBER_READ_AHEAD_EVICTED\",         #?\n",
    "    \"extra_info.global_status.information_schema.innodb_buffer_pool_stats.LRU_IO_TOTAL\",\n",
    "    \"extra_info.global_status.information_schema.innodb_buffer_pool_stats.LRU_IO_CURRENT\",\n",
    "#    \"extra_info.global_status.information_schema.innodb_buffer_pool_stats.UNCOMPRESS_TOTAL\",\n",
    "#    \"extra_info.global_status.information_schema.innodb_buffer_pool_stats.UNCOMPRESS_CURRENT\",\n",
    "]\n",
    "\n",
    "ACTION_MINMAX = (-1,1)  # The Min and Max action values. Arms count and the position of the \"STAY\" arm are depending on it.\n",
    "ON_TERMINATE=-1 # Environement option. -1=Never teminate (the default, just truncate on Timelimit), 0=Terminate with Zero bonus on reward, N=Terminate with a bonus added to the reward (may exceed 1)\n",
    "TRAINING_COVERAGE=1.2 # Coverage of the total states present in dataset. 1.0 means that almost all the dataset will be covered (depends also on the random distribution that may repeat twice the same state)\n",
    "TRAINING_STEPS_PER_EPISODE = 64    # Important to take benefit of the decreasing exploration vs explotation tradeoff\n",
    "TRAINING_VERBOSITY=5\n",
    "NORMALIZE_OBS=False # Sounds a bad idea to normalize with LinUCB\n",
    "\n",
    "\n",
    "# Explore vs exploit\n",
    "ALPHA=0.99 #0.5 \n",
    "\n",
    "# In Labo mode, we consider that having usage of client's latency in LinUCB context, in both learning and test, is not an issue\n",
    "# In Prod mode, we can access client's latency only during learning process. Latency cannot be part of the LinUCB context. The only viable and comparable baseline is BFR (Buffer Filling Rate) baseline.\n",
    "LABO_EXPE=False\n",
    "\n",
    "# Labo or Prod ?\n",
    "if LABO_EXPE:\n",
    "    TRAINING_USE_SLA_PROTECT=True #use or NOT Use the reactive mode to protect the SLA. Let see where we are going without this protection...\n",
    "    TEST_USE_SLA_PROTECT = True #True if \"sysbench_filtered.latency_mean\" in OBSERVATION_SPACE_ELEMS else False\n",
    "else:\n",
    "    TRAINING_USE_SLA_PROTECT=False #It makes no sense to put it at True if we cannot use it in test..\n",
    "    TEST_USE_SLA_PROTECT = False   # Must be like that (False)\n",
    "    OBSERVATION_SPACE_ELEMS.remove(\"sysbench_filtered.latency_mean\") # Ensure latency is not used in context\n",
    "\n",
    "# Discrete reward is either 0 or 1 depending on the Arm (is it the good one or not?). \n",
    "# Continous reward in range [-1., 1.] is computed. The best arm is the one having the maximum value\n",
    "reward_learning = rw.ADBMSBufferCacheRewardContinousV3E(action_minmax=ACTION_MINMAX)\n",
    "reward_test = rw.ADBMSBufferCacheRewardContinousV3E(action_minmax=(-1,1))\n",
    "reward_hpa = rw.ADBMSBufferCacheRewardContinousV3E(action_minmax=(-63,63)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIGMA_TIPPING_RATIO=-1. #-1 #0.9375 # The sigma value for the random gaussian distribution around the \"mean\" (or \"mu\"), the point of SLA tipping. If <0 use the auto-computing based on tipping point.\n",
    "# 1- UCB confidence is <0 !!??\n",
    "#ds_train = ds.ADBMSBufferCacheStatesFullRandomSelector(datasets.df_train, sigma_tipping_ratio=SIGMA_TIPPING_RATIO, seed=RND_SEED)\n",
    "# 2- ok but longer than 3\n",
    "ds_train = ds.ADBMSBufferCacheStatesRandomBufferSelector(datasets.df_train, sigma_tipping_ratio=SIGMA_TIPPING_RATIO, seed=RND_SEED)\n",
    "# 3- sligthly better than ds_train5\n",
    "#ds_train = ds.ADBMSBufferCacheStatesSequentialSelector(datasets.df_train, topdown=True) # At each reset choose the workload from the current episode index, go to the top or bottom state of the workload as specified by topdown argument.\n",
    "# 4- Optimal ?\n",
    "# Choix PR\n",
    "#ds_train = ds.ADBMSBufferCacheStatesFullSequentialSelector(datasets.df_train)  # Start at the begining of the dataset and go to the next state at each reset. When bottom state is reached, next reset will go to the next workload (auto rewind) \n",
    "# 5-\n",
    "#ds_train = ds.ADBMSBufferCacheStatesTopDownRockerSelector(datasets.df_train)            # Start sequentialy each workload twice. First from Top, second from Bottom\n",
    "\n",
    "ds_train.prepareContext(OBSERVATION_SPACE_ELEMS, normalize=NORMALIZE_OBS)\n",
    "\n",
    "K_ARMS = reward_learning.actions.count()\n",
    "K_ARM_STAY = reward_learning.actions.armStay()          # The index of the \"STAY\" arm\n",
    "print(f'K_ARMS:{K_ARMS} STAY_ARM_IDX:{K_ARM_STAY}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from policy_linucb_kfoofw import LinUCBPolicy_kfoofw\n",
    "from policy_linucb_hs import HSLinUCBPolicy\n",
    "\n",
    "# Creates an environement that will change the workload at each Episode and perform only 1 step per episode (better for Exploration vs Exploitation random distribution)\n",
    "env_train = VSMonitor(VSEnv(ds_train, reward=reward_learning, notify_react=TRAINING_USE_SLA_PROTECT, max_steps_per_episode=TRAINING_STEPS_PER_EPISODE, on_terminate=ON_TERMINATE, verbose=0))\n",
    "\n",
    "#policy = LinUCBPolicy_kfoofw(reward_learning.actions, ctx=ds_train.contextElems(), alpha=ALPHA)\n",
    "policy = HSLinUCBPolicy(reward_learning.actions, ctx=ds_train.contextElems(), alpha=ALPHA)\n",
    "\n",
    "linucb_agent = VSAgent(policy)\n",
    "linucb_agent.learn(env_train, TRAINING_COVERAGE, verbose=TRAINING_VERBOSITY)\n",
    "env_train.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linucb_agent.showLearnFig(head_title=\"LinUCB-KFOOFW\", renderer=PLOTLY_RENDERER)\n",
    "linucb_agent.showLearnFig(head_title=f\"LEARN/{policy.name} TotalSteps:{env_train.unwrapped.total_steps}\", renderer=None)\n",
    "linucb_agent.save(filever=str(VERSION), verbose=1, min_max_scaler=ds_train.min_max_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "The DataSet Selector for Tests is a sewuential one. It parses workloads and buffer sizes sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds_test = ds.ADBMSBufferCacheStatesSequentialSelector(datasets.df_test, topdown=True)  # Start sequentialy each Workload at Top or from Bottom depending on \"topdown\" argument\n",
    "ds_test = ds.ADBMSBufferCacheStatesTopDownRockerSelector(datasets.df_test)               # Start sequentialy each workload twice. First from Top, second from Bottom\n",
    "\n",
    "TEST_EPISODES_COUNT = -2    # If <0, abs(EPISODES_COUNT) is a multiple of workloads count \n",
    "TEST_STEPS_PER_EPISODE = -1 # If <0, abs(STEPS_PER_EPISODE) is a multiple of buffer values count \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinUCB Test\n",
    "\n",
    "With learned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linucb_agent = VSAgent(LinUCBPolicy_kfoofw(actions=ACTION_MINMAX))\n",
    "#files_list, optdict = linucb_agent.load(filever=str(VERSION), verbose=1)\n",
    "#ds_test.prepareContext(OBSERVATION_SPACE_ELEMS, normalize=NORMALIZE_OBS, with_scaler=optdict[\"min_max_scaler\"]) # \"with_scaler\" is ignored if No normalization is enabled\n",
    "ds_test.prepareContext(OBSERVATION_SPACE_ELEMS, normalize=NORMALIZE_OBS, with_scaler=ds_train.min_max_scaler) # \"with_scaler\" is ignored if No normalization is enabled\n",
    "\n",
    "# Environement for test with a Reward (coming from the learning sessions). \n",
    "env_test = VSMonitor(VSEnv(ds_test, reward=reward_learning, notify_react=TEST_USE_SLA_PROTECT, max_steps_per_episode=TEST_STEPS_PER_EPISODE, verbose=1))\n",
    "\n",
    "linucb_agent.predict(env_test, episodes_max=TEST_EPISODES_COUNT)\n",
    "\n",
    "\n",
    "graph_tests = env_test.graphPerWorkload(f\"Cumulated Regret/workload. Trained on: {env_train.unwrapped.msgStatus()} {env_train.unwrapped.desc()}\")\n",
    "graph_tests.addCurve(f\"{linucb_agent.policy.name} CReg:{round(env_test.getRegretPerformance(),2)}\", y=env_test.resultPerWorkload(), perf=env_test.getRegretPerformance())\n",
    "\n",
    "env_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linucb_agent.showPredictFig(head_title=\"KFOOFW LinUCB\", renderer=PLOTLY_RENDERER)\n",
    "#linucb_agent.showPredictFig(head_title=f\"TEST/{policy.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline test with HPA (threshold detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LABO_EXPE:\n",
    "    HPA_OBSERVATION_SPACE_ELEMS = [\n",
    "        \"buf_size_idx\",                      # Index form of the buffer size value [0, 63]\n",
    "        \"sysbench_filtered.latency_mean\",    # [0, 1000] ms. Filtered latency from client. In inference/live mode, it should be a predicted value (see predictor.ipynb using XGBoost)\n",
    "        \"latency_threshold\",                 # Latency threshold for the SLA\n",
    "    ]\n",
    "    ext_hpa_threshold = None\n",
    "    hpatyp = \"Lat\"\n",
    "else:\n",
    "    HPA_OBSERVATION_SPACE_ELEMS = [\n",
    "        \"buf_size_idx\",                      # Index form of the buffer size value [0, 63]\n",
    "        \"observation.cache_used_pages_ratio\",    # Buffer cache fill rate in [0., 1.]\n",
    "    ]\n",
    "    ext_hpa_threshold = 0.99\n",
    "    hpatyp = f\"Buf{ext_hpa_threshold}\"\n",
    "\n",
    "from policy_baselines import HpaPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test.prepareContext(HPA_OBSERVATION_SPACE_ELEMS)\n",
    "\n",
    "env_test = VSMonitor(VSEnv(ds_test, reward=reward_hpa, notify_react=TEST_USE_SLA_PROTECT, max_steps_per_episode=TEST_STEPS_PER_EPISODE, verbose=1))\n",
    "\n",
    "policy = HpaPolicy(reward_hpa.actions, ctx=ds_test.contextElems(), ext_threshold=ext_hpa_threshold, debug=False)\n",
    "hpa_agent = VSAgent(policy)\n",
    "# No traning with this policy\n",
    "# Test it!\n",
    "hpa_agent.predict(env_test, episodes_max=TEST_EPISODES_COUNT)\n",
    "\n",
    "graph_tests.addCurve(f\"{policy.name}-{hpatyp} CReg:{round(env_test.getRegretPerformance(),2)}\", y=env_test.resultPerWorkload(), perf=env_test.getRegretPerformance())\n",
    "\n",
    "env_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hpa_agent.showPredictFig(head_title=f\"TEST/{policy.name}\", renderer=PLOTLY_RENDERER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline test with Basic policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_OBSERVATION_SPACE_ELEMS = [\n",
    "    \"sysbench_filtered.latency_mean\",    # [0, 1000] ms. Filtered latency from client. In inference/live mode, it should be a predicted value (see predictor.ipynb using XGBoost)\n",
    "    \"latency_threshold\",                 # Latency threshold for the SLA\n",
    "    \"objective_gap\",\n",
    "    \"latency_mean_min\",\n",
    "    \"latency_mean_max\",\n",
    "]\n",
    "\n",
    "from policy_baselines import BasicPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LABO_EXPE:\n",
    "    ds_test.prepareContext(BASIC_OBSERVATION_SPACE_ELEMS)\n",
    "\n",
    "    env_test = VSMonitor(VSEnv(ds_test, reward=reward_test, notify_react=TEST_USE_SLA_PROTECT, max_steps_per_episode=TEST_STEPS_PER_EPISODE, verbose=1))\n",
    "\n",
    "    policy = BasicPolicy(k_arms=3, stay_arm=K_ARM_STAY, ctx=ds_test.contextElems(), debug=False)\n",
    "    basic_agent = VSAgent(policy)\n",
    "    # No traning with this policy\n",
    "    # Test it!\n",
    "    basic_agent.predict(env_test, episodes_max=TEST_EPISODES_COUNT)\n",
    "    \n",
    "    graph_tests.addCurve(f\"{policy.name} CReg:{round(env_test.getRegretPerformance(),2)}\", y=env_test.resultPerWorkload(), perf=env_test.getRegretPerformance())\n",
    "\n",
    "    env_test.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline test with BufFillingRate Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BFR_OBSERVATION_SPACE_ELEMS = [\n",
    "    \"observation.cache_used_pages_ratio\"\n",
    "]\n",
    "\n",
    "from policy_baselines import BufFillingRatePolicy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test.prepareContext(BFR_OBSERVATION_SPACE_ELEMS)\n",
    "\n",
    "env_test = VSMonitor(VSEnv(ds_test, reward=reward_test, notify_react=TEST_USE_SLA_PROTECT, max_steps_per_episode=TEST_STEPS_PER_EPISODE, verbose=1))\n",
    "\n",
    "policy = BufFillingRatePolicy(reward_test.actions, ctx=ds_test.contextElems(), threshold=0.994, debug=False)\n",
    "bfr_agent = VSAgent(policy)\n",
    "# No traning with this policy\n",
    "# Test it!\n",
    "bfr_agent.predict(env_test, episodes_max=TEST_EPISODES_COUNT)\n",
    "\n",
    "graph_tests.addCurve(f\"{policy.name} CReg:{round(env_test.getRegretPerformance(),2)}\", y=env_test.resultPerWorkload(), perf=env_test.getRegretPerformance())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bfr_agent.showPredictFig(head_title=\"TEST/Basic\", renderer=PLOTLY_RENDERER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = graph_tests.figure()\n",
    "fig.show(renderer=None) # svg is cleaner but not in Gitlab. None is the best in VSCode\n",
    "fig.show(renderer=PLOTLY_RENDERER) # svg is cleaner but not in Gitlab. None is the best in VSCode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
